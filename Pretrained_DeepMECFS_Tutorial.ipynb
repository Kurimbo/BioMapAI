{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b0ccbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Hardware Requirements ###\n",
      "CPU: x86_64\n",
      "Total RAM: 810.20 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import platform\n",
    "\n",
    "print(\"### Hardware Requirements ###\")\n",
    "print(f\"CPU: {platform.processor()}\")\n",
    "print(f\"Total RAM: {psutil.virtual_memory().total / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02339203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 13:20:34.362708: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-11 13:20:35.694707: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Software Requirements ###\n",
      "Python Version: 3.9.13\n",
      "TensorFlow Version: 2.12.0\n",
      "Pandas Version: 1.5.0\n",
      "Numpy Version: 1.24.0\n",
      "sklearn Version: 1.1.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sklearn\n",
    "import platform\n",
    "\n",
    "print(\"### Software Requirements ###\")\n",
    "print(f\"Python Version: {platform.python_version()}\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"Numpy Version: {np.__version__}\")\n",
    "print(f\"sklearn Version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c300b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e948031",
   "metadata": {},
   "source": [
    "# DeepMECFS Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the **DeepMECFS** pretrained BioMapAI model for ME/CFS metabolomics data. The pretrained model is designed to:\n",
    "1. **Load the trained deep learning model (DeepMECFS)**\n",
    "2. **Load the secondary model (Y2y_model)** for final label predictions\n",
    "3. **Align and preprocess** your metabolomics data to match the model’s requirements\n",
    "4. **Compute metrics** (accuracy, precision, recall, F1, AUC, AUCPR) on your dataset\n",
    "\n",
    "## Reference\n",
    "The metabolomics data used in this example:\n",
    "- *\"Plasma metabolomics reveals disrupted response and recovery following maximal exercise in myalgic encephalomyelitis/chronic fatigue syndrome\"*, Arnaud Germain et al., JCI Insight. 2022;7(9):e157621. [DOI: [10.1172/jci.insight.157621](https://doi.org/10.1172/jci.insight.157621)]\n",
    "\n",
    "## Notebook Overview\n",
    "1. **Environment Setup**: Imports and helper functions\n",
    "2. **Load Pretrained Model**: Load the DeepMECFS model and the Y2y_model\n",
    "3. **Load and Preprocess Data**: Demonstration of alignment and scaling\n",
    "4. **Predict & Evaluate**: Use the pretrained model to produce predictions and compute metrics\n",
    "\n",
    "> **Note**: This notebook relies on a folder `pretrained_model_DeepMECFS/` containing:\n",
    ">\n",
    "> - `DeepMECFS_metabolome/` (the main Keras model)\n",
    "> - `Y2y_metabolome/` (the final conversion model)\n",
    "> - `metabolome_feature_metadata.csv` (feature requirement for the model)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cc76b3",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "We’ll begin by importing the necessary Python libraries and defining a helper function to dynamically import **BioMapAI.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c47feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete.\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, average_precision_score\n",
    ")\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "\n",
    "# Clear any previous TensorFlow session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def import_module_with_full_path(file_path):\n",
    "    base_filename = os.path.basename(file_path)\n",
    "    module_name = os.path.splitext(base_filename)[0]\n",
    "    module_spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    imported_module = importlib.util.module_from_spec(module_spec)\n",
    "    module_spec.loader.exec_module(imported_module)\n",
    "    return imported_module\n",
    "\n",
    "print(\"Environment setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e199dff3",
   "metadata": {},
   "source": [
    "## 2. Load Pretrained Model\n",
    "Here we load:\n",
    "- **DeepMECFS_model**: The trained BioMapAI model for ME/CFS metabolomics.\n",
    "- **Y2y_model**: A secondary model that converts the intermediate outputs (Y) into the final binary outcome (`CFS` vs. `Control`).\n",
    "\n",
    "We also load `metabolome_feature_metadata.csv` to ensure our dataset columns align properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffbebac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DeepMECFS_model, Y2y_model, and feature metadata.\n"
     ]
    }
   ],
   "source": [
    "# Import BioMapAI from your local path\n",
    "BioMapAI = import_module_with_full_path(\"BioMapAI.py\")\n",
    "\n",
    "# Load pretrained model\n",
    "DeepMECFS_model = tf.keras.models.load_model(\"pretrained_model_DeepMECFS/DeepMECFS_metabolome/\")\n",
    "Y2y_model = tf.keras.models.load_model(\"pretrained_model_DeepMECFS/Y2y_metabolome/\")\n",
    "feature_meta = pd.read_csv(\"pretrained_model_DeepMECFS/metabolome_feature_metadata.csv\", index_col=0)\n",
    "\n",
    "print(\"Loaded DeepMECFS_model, Y2y_model, and feature metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213b5a9",
   "metadata": {},
   "source": [
    "## 3. Load and Preprocess Data\n",
    "In this example, we load data from an Excel file (`jci.insight.2022.xlsx`). The data includes rows with metabolite measurements and some metadata columns.\n",
    "\n",
    "### Steps:\n",
    "1. **Load the Excel file**.\n",
    "2. **Parse feature metadata** from the first rows/columns.\n",
    "3. **Transpose** and align data columns to match `feature_meta` from the pretrained model.\n",
    "4. **Scale** data using `StandardScaler`.\n",
    "5. **Create** a label vector (`y`) from a `Phenotype` column (mapping `Control` to 0, `CFS` to 1).\n",
    "\n",
    "> **Tip**: Make sure your data columns match the exact names (or COMP_IDs) the pretrained model expects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d52771e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and reformatted.\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "# (Note: You may need 'openpyxl' installed to read Excel files: pip install openpyxl)\n",
    "data = pd.read_excel(\"example_data/jci.insight.2022.xlsx\", sheet_name='ScaledImpDataZeroDrug&Tobacco')\n",
    "\n",
    "# The following transformations align the dataset to match the pretrained model's requirements\n",
    "# 1) Separate feature metadata\n",
    "data_feature_meta = data.iloc[6:, :15]\n",
    "data_feature_meta.columns = data_feature_meta.iloc[0]\n",
    "data_feature_meta.index = data_feature_meta.loc[:, 'COMP ID']\n",
    "data_feature_meta = data_feature_meta.drop('COMP ID')\n",
    "\n",
    "# 2) Extract the sample metadata (Phenotype, etc.) from columns 15 onward\n",
    "meta = data.iloc[:6, 15:].transpose()\n",
    "meta.columns = meta.iloc[0]\n",
    "meta = meta.drop('ID')\n",
    "\n",
    "# 3) The main data matrix starts at row 7, column 16\n",
    "data_main = data.iloc[7:, 16:]\n",
    "data_main.index = data_feature_meta.index\n",
    "data_main.columns = meta.index\n",
    "\n",
    "# 4) Transpose so rows = samples, columns = metabolites\n",
    "data_main = data_main.transpose()\n",
    "\n",
    "# 5) Reindex the feature metadata for easy overlap calculations\n",
    "feature_meta.index = feature_meta.COMP_ID\n",
    "\n",
    "print(\"Data loaded and reformatted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d94ba4",
   "metadata": {},
   "source": [
    "### Check Feature Coverage\n",
    "Next, we ensure that the features in your dataset overlap with the ones expected by the pretrained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49476af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model features: 730\n",
      "Test dataset features: 1157\n",
      "Overlap: 573\n",
      "Feature Coverage: 0.7849315068493151\n"
     ]
    }
   ],
   "source": [
    "overlap_features = set(data_main.columns).intersection(feature_meta.index)\n",
    "overlap_len = len(overlap_features)\n",
    "\n",
    "print(\"Model features:\", len(feature_meta.index))\n",
    "print(\"Test dataset features:\", len(data_main.columns))\n",
    "print(\"Overlap:\", overlap_len)\n",
    "print(\"Feature Coverage:\", overlap_len / len(feature_meta.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873a4c9",
   "metadata": {},
   "source": [
    "### Finalize X (features) and y (labels)\n",
    "1. **Reindex** the data columns to match `feature_meta` order.\n",
    "2. **Fill missing features** (if any) with zeros.\n",
    "3. **Extract** the phenotype column from `meta` and map to 0 or 1.\n",
    "4. **Scale** the data with `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29356031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features and labels prepared.\n"
     ]
    }
   ],
   "source": [
    "# Align columns\n",
    "X = data_main.reindex(columns=feature_meta.index, fill_value=0)\n",
    "\n",
    "# Convert data to float\n",
    "X = X.astype(\"float32\")\n",
    "\n",
    "# Extract labels (y)\n",
    "y = meta.Phenotype.map({'Control': 0, 'CFS': 1})\n",
    "y = y.astype(\"float32\")\n",
    "\n",
    "# Scale features\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "print(\"Features and labels prepared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd11b07",
   "metadata": {},
   "source": [
    "## 4. Predict & Evaluate\n",
    "Use **BioMapAI.ScoreYModel** to combine your pretrained **DeepMECFS_model** and **Y2y_model**, then generate predictions and compute various metrics.\n",
    "\n",
    "### `calculate_metrics` Function\n",
    "We have defined `calculate_metrics` to compute:\n",
    "- **Accuracy**\n",
    "- **Precision**\n",
    "- **Recall**\n",
    "- **F1-Score**\n",
    "- **AUC** (Area Under the ROC Curve)\n",
    "- **AUCPR** (Area Under the Precision-Recall Curve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d9ba2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function for metrics defined.\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    aucpr = average_precision_score(y_true, y_pred)\n",
    "    result = pd.Series([\n",
    "        accuracy, precision, recall, f1, auc, aucpr\n",
    "    ], index=['accuracy','precision','recall','f1','auc','aucpr'])\n",
    "    return result\n",
    "\n",
    "print(\"Helper function for metrics defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c3b72b",
   "metadata": {},
   "source": [
    "### 4.1 Generate Predictions and Metrics\n",
    "1. **Predict probabilities** (`y_prob`) with `ScoreYModel.predict()`.\n",
    "2. Convert probabilities to hard predictions (`y_pred`) using a threshold of 0.5.\n",
    "3. Evaluate using the `calculate_metrics` function.\n",
    "4. Compute **loss** and **accuracy** from the model’s `.evaluate()`.\n",
    "5. Optionally, compute a continuous score with `.get_score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62607a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 2ms/step\n",
      "13/13 [==============================] - 0s 824us/step\n",
      "Metrics on your dataset:\n",
      "accuracy     0.581683\n",
      "precision    0.605536\n",
      "recall       0.760870\n",
      "f1           0.674374\n",
      "auc          0.552849\n",
      "aucpr        0.596873\n",
      "dtype: float64\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "13/13 [==============================] - 0s 810us/step\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.1390 - accuracy: 0.5842\n",
      "\n",
      "Model Evaluation:\n",
      "Loss: 1.1389771699905396\n",
      "Accuracy: 0.5841584205627441\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "\\Score (example): [[0.69249278 4.         2.         0.90231776 0.34845942 0.71620554\n",
      "  0.52002591 0.77187139 0.48639807 0.69270623 0.59847623 0.38456982]\n",
      " [0.65149671 4.         3.         0.89950168 0.36542782 0.69211692\n",
      "  0.51083148 0.7390132  0.47947568 0.70352364 0.57209647 0.34919432]\n",
      " [0.54803205 4.         0.         0.86284113 0.30350617 0.58225155\n",
      "  0.41387561 0.64547694 0.45103189 0.66739452 0.49009156 0.28886947]\n",
      " [0.09153847 0.         0.         0.71719098 0.29834646 0.25541073\n",
      "  0.20815381 0.37611681 0.38354927 0.61935139 0.28919482 0.02028154]\n",
      " [0.68932599 4.         0.         0.90346587 0.34127256 0.7179293\n",
      "  0.51207089 0.76941878 0.48757833 0.69147861 0.59413648 0.36833206]]\n"
     ]
    }
   ],
   "source": [
    "# Generate probabilities\n",
    "y_prob = BioMapAI.ScoreYModel(DeepMECFS_model, Y2y_model).predict(X)\n",
    "\n",
    "# Binarize predictions\n",
    "y_pred = (y_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "# Compute metrics\n",
    "metrics_result = calculate_metrics(y, y_pred)\n",
    "print(\"Metrics on your dataset:\")\n",
    "print(metrics_result)\n",
    "\n",
    "# Evaluate final output (Keras-style loss & accuracy)\n",
    "loss, accuracy = BioMapAI.ScoreYModel(DeepMECFS_model, Y2y_model).evaluate(X, y)\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# (Optional) Get a score instead of probability\n",
    "score = BioMapAI.ScoreYModel(DeepMECFS_model, Y2y_model).get_score(X)\n",
    "print(\"\\Score (example):\", score[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c49fe3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "You have successfully:\n",
    "1. Loaded the **DeepMECFS** pretrained model and **Y2y_model**.\n",
    "2. Aligned, preprocessed, and scaled your metabolomics data.\n",
    "3. Generated predictions for ME/CFS vs. Control classification.\n",
    "4. Evaluated using various metrics (accuracy, precision, recall, F1, AUC, AUCPR).\n",
    "\n",
    "Feel free to adapt the code to your own datasets. Keep in mind:\n",
    "- You must match the exact features the model expects.\n",
    "- Scaling or normalization steps should be consistent.\n",
    "- Additional hyperparameter tuning is not necessary here since the model is pretrained, but you can still experiment with thresholding or post-processing.\n",
    "\n",
    "Happy modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ced1e65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time: 10.36 seconds\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "print(f\"Total execution time: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
